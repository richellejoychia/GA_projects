{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041a2393-626b-444a-a434-2416d9d067ab",
   "metadata": {},
   "source": [
    "# Project 3(1): Classifying Reddit Posts with Natural Language Processing and Machine Learning - Scrap Subreddits Text\n",
    "\n",
    "Done by: Richelle-Joy Chia, a Redditor-and-data-science enthusiast! \n",
    "\n",
    "Problem statement: Through natural language processing and classification models, how can we help Reddit and other interested parties classify posts based on the texts used by people who may be depressed or anxious? Furthermore, how can sentiment analysis be utilized to detect emotions associated with depression and anxiety?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf414d-fcc1-4b86-88ae-11c25921804f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Mental illness is highly prevalent worldwide, constituting a major cause of distress in people's life with impact on society's health and well-being. Depression and anxiety are psychiatric disorders that are observed in many areas of everyday life and affect millions of people worldwide. \n",
    "These mental disorders manifest somewhat frequently in texts written by non diagnosed users on social media. Furthermore, social stigma around mental disorders influenced people to not speak out and turn to other means (eg. social platforms). \n",
    "\n",
    "For the scope of this project, I focused on scraping texts from Reddit, which is one of the common platforms for people to explode and share their thoughts. Thereafter, I ran multiple classification models (LogReg, Naive Bayes, and Random Forest Classifier) and sentiment analysis (Hugging Face) to analyse the texts. For the classification models, I used the accuracy scores and confusion matrix as the metrics for sifting out the best model. As for the sentiment analysis, I used the probability as the metric for determining which model is better.\n",
    "\n",
    "These are the two subreddits:\n",
    "\n",
    "- Anxiety subreddit: https://www.reddit.com/r/Anxiety/\n",
    "- Depression subreddit: https://www.reddit.com/r/depression/\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "As such, it would be interesting to explore how we can help Reddit and other interested parties classify texts based on the differences in words used by people who suffer from depression and anxiety through natural language processing and classification models? Furthermore, how can sentiment analysis be utilized to detect emotions associated with depression and anxiety? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0e69a-c34b-4a04-b1ab-71679e092089",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "    \n",
    "- Part 1: Introduction and Data Scraping\n",
    "- Part 2: Data Cleaning\n",
    "- Part 3: Preprocessing\n",
    "- Part 4: Modeling \n",
    "- Part 5a-b: Exploratory Analysis with 2 models from Hugging Face\n",
    "    - Model 1: https://huggingface.co/j-hartmann/emotion-english-roberta-large\n",
    "    - Model 2: https://huggingface.co/arpanghoshal/EmoRoBERTa\n",
    "- Part 5c: Hugging Face Models Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01240a-df21-4ef9-bffc-2ed72aa00cc4",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778e41aa-1539-4d71-9497-d33a13cab9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e22f8f-1d0c-4ab1-b302-2131b22c1ad5",
   "metadata": {},
   "source": [
    "#### Data scraping will begin from Friday as it is usually the day that posts tend to get deleted by moderators for various reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae8e786-5625-462b-bd7a-fcc5555cda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom a function to grab posts from the subreddits\n",
    "\n",
    "def getposts(subreddit,utc_timing=1664755200): #this timing is set at Monday 12.00am. Start to filter from Sunday backwards.\n",
    "\n",
    "    # posts_df = pd.DataFrame(columns=['subreddit', 'selftext', 'title']) #create an empty dataframe. \n",
    "    timing = utc_timing #set timing to the current time which I added on the function\n",
    "    posts_number = 0 #set posts_number to zero. the loop will make use of this.\n",
    "    total_post = [] #create an empty list for the post\n",
    "\n",
    "    while True:\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission' #base site, params will add the subreddit\n",
    "        params = {\n",
    "            'subreddit': subreddit, #the subreddit which i have to key in\n",
    "            'size' : 250, #max i can go\n",
    "            'before' : timing #the first time will be the one above. subsequent timing will be a week before.\n",
    "        }\n",
    "        print(f'Searching through {subreddit}. Total post so far: {posts_number}') #print this to show its running\n",
    "        res = requests.get(url, params) #standard stuff. requesting url with the params\n",
    "        data = res.json() #save the website stuff into json and then use data to store it.\n",
    "        posts = data['data'] #inside 'data' in data is where the informatiom we want.\n",
    "        \n",
    "        \n",
    "        for i in range(len(posts)):\n",
    "            post = {}\n",
    "            epoch_time = posts[i]['created_utc'] #find the [i] post epoch time\n",
    "            date_time = datetime.datetime.fromtimestamp( epoch_time ) #convert the epoch time to standard time\n",
    "            #I saved the date time as i want to ensure the posts pulled are correct. The time is a good reference.     \n",
    "            try:\n",
    "                post['date_time'] = date_time.strftime('%Y-%m-%d %H:%M:%S') #save to a format which is readable\n",
    "            except:\n",
    "                post['date_time'] = 'post deleted' #incase posts get deleted and throws an error\n",
    "            \n",
    "            try:\n",
    "                post['subreddit'] = posts[i]['subreddit'] #save the subreddit name\n",
    "            except:\n",
    "                post['subreddit'] = 'post deleted' #incase posts get deleted and throws an error\n",
    "            try:\n",
    "                post['selftext'] = posts[i]['selftext'] #save the selftext data into selftext\n",
    "            except:\n",
    "                post['selftext'] = 'post deleted' #incase posts get deleted and throws an error\n",
    "            try: \n",
    "                post['title'] = posts[i]['title'] #save the post title into title\n",
    "            except:\n",
    "                post['title'] = 'post deleted' #incase posts get deleted and throws an error\n",
    "           \n",
    "            \n",
    "            total_post.append(post) #append this current post dictionary, and put it ito total_post list.\n",
    "\n",
    "        \n",
    "        posts_number += len(posts) #add to post_number for counter and for the while true loop.\n",
    "        max_post = len(post)\n",
    "        timing = posts[-1]['created_utc'] #timing is set to the earliest post in the posts list\n",
    "        #in a day. so minus a week have a better time range all the way to last year for the 15,000 post\n",
    "\n",
    "        if posts_number >= 15000: #if this is true, the while loop breaks\n",
    "            break\n",
    "    \n",
    "    posts_df = pd.DataFrame(total_post) #save the total_post from the subreddit and place it in a posts_df(dataframe)\n",
    "    posts_df.to_csv('./dataset/'+subreddit+'.csv') #save the file name into dataset folder, with the subreddit name.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24068c3-5205-40c5-bb7c-b79a69b6a0fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching through Anxiety. Total post so far: 0\n",
      "Searching through Anxiety. Total post so far: 250\n",
      "Searching through Anxiety. Total post so far: 500\n",
      "Searching through Anxiety. Total post so far: 750\n",
      "Searching through Anxiety. Total post so far: 1000\n",
      "Searching through Anxiety. Total post so far: 1250\n",
      "Searching through Anxiety. Total post so far: 1500\n",
      "Searching through Anxiety. Total post so far: 1750\n",
      "Searching through Anxiety. Total post so far: 1999\n",
      "Searching through Anxiety. Total post so far: 2249\n",
      "Searching through Anxiety. Total post so far: 2499\n",
      "Searching through Anxiety. Total post so far: 2749\n",
      "Searching through Anxiety. Total post so far: 2999\n",
      "Searching through Anxiety. Total post so far: 3249\n",
      "Searching through Anxiety. Total post so far: 3499\n",
      "Searching through Anxiety. Total post so far: 3748\n",
      "Searching through Anxiety. Total post so far: 3998\n",
      "Searching through Anxiety. Total post so far: 4248\n",
      "Searching through Anxiety. Total post so far: 4498\n",
      "Searching through Anxiety. Total post so far: 4748\n",
      "Searching through Anxiety. Total post so far: 4998\n",
      "Searching through Anxiety. Total post so far: 5248\n",
      "Searching through Anxiety. Total post so far: 5498\n",
      "Searching through Anxiety. Total post so far: 5748\n",
      "Searching through Anxiety. Total post so far: 5998\n",
      "Searching through Anxiety. Total post so far: 6248\n",
      "Searching through Anxiety. Total post so far: 6498\n",
      "Searching through Anxiety. Total post so far: 6748\n",
      "Searching through Anxiety. Total post so far: 6998\n",
      "Searching through Anxiety. Total post so far: 7248\n",
      "Searching through Anxiety. Total post so far: 7498\n",
      "Searching through Anxiety. Total post so far: 7748\n",
      "Searching through Anxiety. Total post so far: 7997\n",
      "Searching through Anxiety. Total post so far: 8247\n",
      "Searching through Anxiety. Total post so far: 8497\n",
      "Searching through Anxiety. Total post so far: 8746\n",
      "Searching through Anxiety. Total post so far: 8996\n",
      "Searching through Anxiety. Total post so far: 9246\n",
      "Searching through Anxiety. Total post so far: 9496\n",
      "Searching through Anxiety. Total post so far: 9746\n",
      "Searching through Anxiety. Total post so far: 9996\n",
      "Searching through Anxiety. Total post so far: 10245\n",
      "Searching through Anxiety. Total post so far: 10495\n",
      "Searching through Anxiety. Total post so far: 10744\n",
      "Searching through Anxiety. Total post so far: 10994\n",
      "Searching through Anxiety. Total post so far: 11244\n",
      "Searching through Anxiety. Total post so far: 11494\n",
      "Searching through Anxiety. Total post so far: 11744\n",
      "Searching through Anxiety. Total post so far: 11994\n",
      "Searching through Anxiety. Total post so far: 12244\n",
      "Searching through Anxiety. Total post so far: 12494\n",
      "Searching through Anxiety. Total post so far: 12744\n",
      "Searching through Anxiety. Total post so far: 12994\n",
      "Searching through Anxiety. Total post so far: 13244\n",
      "Searching through Anxiety. Total post so far: 13494\n",
      "Searching through Anxiety. Total post so far: 13744\n",
      "Searching through Anxiety. Total post so far: 13994\n",
      "Searching through Anxiety. Total post so far: 14244\n",
      "Searching through Anxiety. Total post so far: 14493\n",
      "Searching through Anxiety. Total post so far: 14743\n",
      "Searching through Anxiety. Total post so far: 14992\n"
     ]
    }
   ],
   "source": [
    "getposts('Anxiety',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43891428-5f7e-4634-a019-5018008d60cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching through Depression. Total post so far: 0\n",
      "Searching through Depression. Total post so far: 250\n",
      "Searching through Depression. Total post so far: 500\n",
      "Searching through Depression. Total post so far: 750\n",
      "Searching through Depression. Total post so far: 1000\n",
      "Searching through Depression. Total post so far: 1250\n",
      "Searching through Depression. Total post so far: 1499\n",
      "Searching through Depression. Total post so far: 1749\n",
      "Searching through Depression. Total post so far: 1999\n",
      "Searching through Depression. Total post so far: 2248\n",
      "Searching through Depression. Total post so far: 2497\n",
      "Searching through Depression. Total post so far: 2745\n",
      "Searching through Depression. Total post so far: 2995\n",
      "Searching through Depression. Total post so far: 3245\n",
      "Searching through Depression. Total post so far: 3495\n",
      "Searching through Depression. Total post so far: 3745\n",
      "Searching through Depression. Total post so far: 3995\n",
      "Searching through Depression. Total post so far: 4245\n",
      "Searching through Depression. Total post so far: 4495\n",
      "Searching through Depression. Total post so far: 4745\n",
      "Searching through Depression. Total post so far: 4995\n",
      "Searching through Depression. Total post so far: 5245\n",
      "Searching through Depression. Total post so far: 5495\n",
      "Searching through Depression. Total post so far: 5745\n",
      "Searching through Depression. Total post so far: 5995\n",
      "Searching through Depression. Total post so far: 6244\n",
      "Searching through Depression. Total post so far: 6494\n",
      "Searching through Depression. Total post so far: 6744\n",
      "Searching through Depression. Total post so far: 6994\n",
      "Searching through Depression. Total post so far: 7244\n",
      "Searching through Depression. Total post so far: 7493\n",
      "Searching through Depression. Total post so far: 7742\n",
      "Searching through Depression. Total post so far: 7992\n",
      "Searching through Depression. Total post so far: 8242\n",
      "Searching through Depression. Total post so far: 8492\n",
      "Searching through Depression. Total post so far: 8742\n",
      "Searching through Depression. Total post so far: 8992\n",
      "Searching through Depression. Total post so far: 9241\n",
      "Searching through Depression. Total post so far: 9491\n",
      "Searching through Depression. Total post so far: 9740\n",
      "Searching through Depression. Total post so far: 9990\n",
      "Searching through Depression. Total post so far: 10240\n",
      "Searching through Depression. Total post so far: 10490\n",
      "Searching through Depression. Total post so far: 10738\n",
      "Searching through Depression. Total post so far: 10988\n",
      "Searching through Depression. Total post so far: 11238\n",
      "Searching through Depression. Total post so far: 11488\n",
      "Searching through Depression. Total post so far: 11738\n",
      "Searching through Depression. Total post so far: 11988\n",
      "Searching through Depression. Total post so far: 12238\n",
      "Searching through Depression. Total post so far: 12488\n",
      "Searching through Depression. Total post so far: 12738\n",
      "Searching through Depression. Total post so far: 12988\n",
      "Searching through Depression. Total post so far: 13238\n",
      "Searching through Depression. Total post so far: 13488\n",
      "Searching through Depression. Total post so far: 13738\n",
      "Searching through Depression. Total post so far: 13988\n",
      "Searching through Depression. Total post so far: 14238\n",
      "Searching through Depression. Total post so far: 14488\n",
      "Searching through Depression. Total post so far: 14738\n",
      "Searching through Depression. Total post so far: 14988\n"
     ]
    }
   ],
   "source": [
    "getposts('Depression',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fc229-7c7e-4cc1-8505-63380d20f83d",
   "metadata": {},
   "source": [
    "## It is time to proceed to data cleaning! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
